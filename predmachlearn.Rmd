---
title: "Coursera - predmachlearn project"
//author: "1va"
date: "July 2014"
output: html_document
---
Data source: http://groupware.les.inf.puc-rio.br/har. (These are measurements from accelerometers on the belt, forearm, arm, and dumbell of 6 participants performing barbell lifts correctly and incorrectly in 5 different ways. Goal is to predict the manner in which they did the exercise.)

Load packages and explore the data:
```{r}
library(caret)
library(gbm)
dat <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
#summary(training)
```
Cleaning data: Check what variables are available in testing set, so that we make prediction based on them:
```{r}
ind <- sapply(testing,function(x) sum(is.na(x))<10)
ind[c(1,3,4,5,6)]<-FALSE
dat<-dat[ind]
testing<-testing[ind]
```
Note that we ended up with 54 explanatory variables (1 of them being a factor).

We have a lot of data so we can put aside a validation set, before building models:
```{r}
set.seed(3433)
inTrain = createDataPartition(dat$classe, p = 1/4)[[1]]
training = dat[ inTrain,]
validating = dat[-inTrain,]
```

Lets try several models (here is just a subset), and use the validation set to estimate accuracy:
```{r}
model1<-train(classe~.,data=training,method='rf', trControl=trainControl(method = "cv"))
model2<-train(classe~.,data=training,method='gbm')
model3<-train(classe~.,data=training,method='lda')
y1<-predict(model1,validating)
y2<-predict(model2,validating)
y3<-predict(model2,validating)

confusionMatrix(y1,validating$classe)$overall[1]
confusionMatrix(y2,validating$classe)$overall[1]
confusionMatrix(y3,validating$classe)$overall[1]
```

I chose the first model, because has a very good accuracy (99%) on the validation set. I would expect the accuracy on the test set to be slightly lower but not by much.

Now export the test answers:
```{r}
y<-predict(model1,testing)
#pml_write_files(y)
```
Hurrah 20/20 :)